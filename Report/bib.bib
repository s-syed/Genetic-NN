@article { adams,
	title = {Learning the Structure of Deep Sparse Graphical Models},
	journal = {Journal of Machine Learning Research: Workshop and Conference Proceedings (AISTATS)},
	volume = {9},
	year = {2010},
	month = {05/2010},
	pages = {1-8},
	abstract = {Deep belief networks are a powerful way to model complex probability distributions. However, learning the structure of a belief network, particularly one with hidden units, is difficult. The Indian buffet process has been used as a nonparametric Bayesian prior on the directed structure of a belief network with a single infinitely wide hidden layer. In this paper, we introduce the cascading Indian buffet process (CIBP), which provides a nonparametric prior on the structure of a layered, directed belief network that is unbounded in both depth and width, yet allows tractable inference. We use the CIBP prior with the nonlinear Gaussian belief network so each unit can additionally vary its behavior between discrete and continuous representations. We provide Markov chain Monte Carlo algorithms for inference in these belief networks and explore the structures learned on several image data sets.},
	keywords = {bayesian nonparametrics, graphical models, highlighted, indian buffet process, structure learning},
	url = {http://hips.seas.harvard.edu/files/w/papers/adams-wallach-ghahramani-2010a.pdf},
	author = {Ryan P. Adams and Hanna M. Wallach and Zoubin Ghahramani}
}

@article{Stanley,
 author = {Stanley, Kenneth O. and Miikkulainen, Risto},
 title = {Evolving Neural Networks Through Augmenting Topologies},
 journal = {Evol. Comput.},
 issue_date = {Summer 2002},
 volume = {10},
 number = {2},
 month = jun,
 year = {2002},
 issn = {1063-6560},
 pages = {99--127},
 numpages = {29},
 url = {http://dx.doi.org/10.1162/106365602320169811},
 doi = {10.1162/106365602320169811},
 acmid = {638554},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
 keywords = {competing conventions, genetic algorithms, network topologies, neural networks, neuroevolution, speciation},
} 

ï»¿@Inbook{Stepniewski,
author="Stepniewski, Slawomir W.
and Keane, Andy J.",
editor="Voigt, Hans-Michael
and Ebeling, Werner
and Rechenberg, Ingo
and Schwefel, Hans-Paul",
chapter="Topology design of feedforward neural networks by genetic algorithms",
title="Parallel Problem Solving from Nature --- PPSN IV: International Conference on Evolutionary Computation --- The 4th International Conference on Parallel Problem Solving from Nature Berlin, Germany, September 22--26, 1996 Proceedings",
year="1996",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="771--780",
isbn="978-3-540-70668-7",
doi="10.1007/3-540-61723-X_1040",
url="http://dx.doi.org/10.1007/3-540-61723-X_1040"
}


@inproceedings{Miller,
 author = {Miller, Geoffrey F. and Todd, Peter M. and Hegde, Shailesh U.},
 title = {Designing Neural Networks Using Genetic Algorithms},
 booktitle = {Proceedings of the Third International Conference on Genetic Algorithms},
 year = {1989},
 isbn = {1-55860-006-3},
 location = {George Mason University, USA},
 pages = {379--384},
 numpages = {6},
 url = {http://dl.acm.org/citation.cfm?id=93126.94034},
 acmid = {94034},
 publisher = {Morgan Kaufmann Publishers Inc.},
 address = {San Francisco, CA, USA},
} 

@inproceedings{Montana,
 author = {Montana, David J. and Davis, Lawrence},
 title = {Training Feedforward Neural Networks Using Genetic Algorithms},
 booktitle = {Proceedings of the 11th International Joint Conference on Artificial Intelligence - Volume 1},
 series = {IJCAI'89},
 year = {1989},
 location = {Detroit, Michigan},
 pages = {762--767},
 numpages = {6},
 url = {http://dl.acm.org/citation.cfm?id=1623755.1623876},
 acmid = {1623876},
 publisher = {Morgan Kaufmann Publishers Inc.},
 address = {San Francisco, CA, USA},
} 

@ARTICLE{Zhang,
    author = {Byoung-tak Zhang and Heinz Muhlenbein},
    title = {Evolving optimal neural networks using genetic algorithms with Occam's razor},
    journal = {Complex Systems},
    year = {1993},
    volume = {7},
    pages = {199--220}
}

@book{Mitchell:1998:IGA:522098,
 author = {Mitchell, Melanie},
 title = {An  Introduction to Genetic Algorithms},
 year = {1998},
 isbn = {0262631857},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 

@article{mnistlecun,
    author = {Lecun, Yann and Cortes, Corinna},
    citeulike-article-id = {599493},
    citeulike-linkout-0 = {http://yann.lecun.com/exdb/mnist/},
    keywords = {ai, mnist, recognition},
    posted-at = {2009-03-20 17:02:13},
    priority = {2},
    title = {{The MNIST database of handwritten digits}},
    url = {http://yann.lecun.com/exdb/mnist/}
}

@inproceedings{sutskever, 
    Publisher = {JMLR Workshop and Conference Proceedings}, 
    Title = {On the importance of initialization and momentum in deep learning}, 
    Url = {http://jmlr.org/proceedings/papers/v28/sutskever13.pdf}, 
    Booktitle = {Proceedings of the 30th International Conference on Machine Learning (ICML-13)}, 
    Author = {Ilya Sutskever and James Martens and George Dahl and Geoffrey Hinton}, 
    Number = {3}, 
    Month = may, 
    Volume = {28}, 
    Editor = {Sanjoy Dasgupta and David Mcallester}, 
    Year = {2013}, 
    Pages = {1139-1147}, 
    Abstract = {Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter, it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimization. We find that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks perform markedly worse when the momentum is absent or poorly tuned. Our success training these models suggests that previous attempts to train deep and recurrent neural networks from random initializations have likely failed due to poor initialization schemes. Furthermore, carefully tuned momentum methods suffice for dealing with the curvature issues in deep and recurrent network training objectives without the need for sophisticated second-order methods. } 
   }

@article{srivastava,
  author  = {Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
  title   = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  journal = {Journal of Machine Learning Research},
  year    = {2014},
  volume  = {15},
  pages   = {1929-1958},
  url     = {http://jmlr.org/papers/v15/srivastava14a.html}
}

